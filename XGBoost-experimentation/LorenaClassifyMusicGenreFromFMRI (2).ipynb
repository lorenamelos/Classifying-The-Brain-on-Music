{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOQbaNfzC097"
      },
      "source": [
        "# Classifying The Brain on Music"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O4hDeeyC0-A"
      },
      "source": [
        "Your objective is to classify fMRI brain images taken while listening to music in five different genres: label 0=Ambient Music, 1=Country Music, 2=Heavy Metal, 3=Rock 'n Roll, 4=Classical Symphonic. The data consists of train_data.csv, train_labels.csv, and test_data.csv, for a one-person subset of a larger 20-subject study, linked above.\n",
        "\n",
        "The training data (train_data.csv) consist of 160 event-related brain images (trials), corresponding to twenty 6-second music clips, four clips in each of the five genres, repeated in-order eight times (runs). The labels (train_labels.csv) correspond to the correct musical genres, listed above, for each of the 160 trials.\n",
        "There are 22036 features in each brain image, corresponding to blood-oxygenation levels at each 2mm-cubed 3D location within a section of the auditory cortex. In human brain imaging, there are often many more features (brain sites) than samples (trials), thus making the task a relatively challenging multiway classification problem.\n",
        "\n",
        "The testing data (test_data.csv) consists of 40 event-related brain images corresponding to novel 6-second music clips in the five genres. The test data is in randomized order with no labels. You must predict, using only the given brain images, the correct genre labels (0-4) for the 40 test trials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25QjbIJtflqc"
      },
      "source": [
        "# Final Project\n",
        "## \"Classifying The Brain on Music\"\n",
        "\n",
        "Michael Casey, https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.01179/full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBkfTOxuWxUY"
      },
      "source": [
        "## 1. Multi-Class Genre Classifier\n",
        "\n",
        "Build a multi-class classifier for the 5 music genres. Your goal is to train a model to classify brain images into corresponding genre categories. You are free to choose any machine learning models from the class.\n",
        "\n",
        "## 1-1. Hyper-parameter Search  \n",
        "Demonstrate your hyperparameter search process using cross-validation. Provide details for at least one hyperparameter with 10 different possible values.\n",
        "\n",
        "## 1-2. Model Training and Testing\n",
        "Following the hyperparameter search, train your model with the best combination of hyperparameters. Run the model on the test set and submit the results to the Kaggle competition. To get full marks, your model should outperform the baseline model, which is provided in Kaggle. You must show your test accuracy computed by Kaggle in this report.\n",
        "\n",
        "## 1-3. Model Analysis\n",
        "Conduct a thorough analysis of your model, including:\n",
        "\n",
        "### 1-3-1. Confusion Matrix:\n",
        "Split the training set into train/validation sets. The data is organized into eight runs, in order, with each run repeating the same 20 music trials. You should split the data by run. Retrain your model using the best hyperparameter combination. Present the confusion matrix on the validation set.\n",
        "\n",
        "### 1-3-2. Example Examination:\n",
        "Examine four validation samples where your model fails to classify into the correct category. Display the true label and the predicted label. Looking at the confusion matrix, how might you explain your results from the perspectives of human brain data and music genre similarity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA9cw0emNkjC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **A. Data Download**\n",
        "### For your convenience, we have provided code to download the dataset, which includes true labels, training data (features), training labels, and testing data (features)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBfZLkUrZ9Xx"
      },
      "source": [
        "###  A-1. Download Features and Labels.**\n",
        "#### Run the following code to download the brain features and labels of the music clips.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2VKsjDVkC0-F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFIXZZX4cQZR",
        "outputId": "b8e7483e-c481-4ee4-8837-ee59ebc6401e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file paths\n",
        "file1_path = \"/content/drive/My Drive/data/train_data.csv\"\n",
        "file2_path = \"/content/drive/My Drive/data/test_data.csv\"\n",
        "file3_path = \"/content/drive/My Drive/data/train_labels.csv\"\n",
        "\n",
        "# Load the data into Pandas DataFrames\n",
        "train_data = pd.read_csv(file1_path)\n",
        "test_data = pd.read_csv(file2_path)\n",
        "tran_labels = pd.read_csv(file3_path)\n"
      ],
      "metadata": {
        "id": "LRJ40lD1eTIB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvMqDITgzRW2"
      },
      "outputs": [],
      "source": [
        "#!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FA1IkzqxNFw7",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bd828d-aa74-4e85-8600-842a80070a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data.shape: (160, 22036)\n",
            "train_labels.shape: (160, 1)\n",
            "test_data.shape: (40, 22036)\n"
          ]
        }
      ],
      "source": [
        "# Data Import Method 1, with pandas\n",
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"train_data.csv\", header=None)\n",
        "train_labels = pd.read_csv(\"train_labels.csv\", header=None)\n",
        "test_data = pd.read_csv(\"test_data.csv\", header=None)\n",
        "\n",
        "print('train_data.shape: {}'.format(train_data.shape))\n",
        "print('train_labels.shape: {}'.format(train_labels.shape))\n",
        "print('test_data.shape: {}'.format(test_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD-ArHgoC0-M"
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(train_data)\n",
        "df_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtElr_9-C0-N"
      },
      "source": [
        " ## labels\n",
        "\n",
        " 0=Ambient Music, 1=Country Music, 2=Heavy Metal, 3=Rock 'n Roll, 4=Classical Symphonic."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame(test_data)\n",
        "df_test"
      ],
      "metadata": {
        "id": "CadCoxJPbcvG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b159b25-8186-4f48-9985-f6bd8d128a8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0         1         2         3         4         5         6      \\\n",
              "0  -0.717852 -1.489977 -0.788490 -0.766279 -0.672843 -0.485007  0.396688   \n",
              "1  -2.750366 -2.888667 -1.469767 -1.804638 -1.798576 -0.161826 -0.319420   \n",
              "2   2.866373  2.680758  1.306910  0.890039 -0.175718 -0.730438 -0.993796   \n",
              "3   0.598113  2.346176  0.877477  3.045216  3.963654  1.601371  3.541291   \n",
              "4   0.567829  0.515177  1.447557  1.440498  1.315393  1.180648  1.389178   \n",
              "5   0.866487  0.280718  0.666179  0.489518 -0.428805  0.316380  0.760662   \n",
              "6   1.484973 -0.054756  2.056316  0.244368 -1.385715  2.112417  0.241465   \n",
              "7  -0.396418  0.134505  0.452111  0.797386  0.627791  0.687820  1.199464   \n",
              "8  -0.088526  0.553960  1.642410  2.349703  1.149499  2.607508  3.245725   \n",
              "9   1.237615 -0.130142  0.775877 -0.234693 -1.319983 -1.043659 -0.677686   \n",
              "10  3.183168  2.770886  0.827897  0.851232 -0.640553 -0.263272 -0.431441   \n",
              "11  0.301304  0.930218  1.412441  2.474402  0.944745  2.213462  3.249793   \n",
              "12 -0.137585  1.486551 -0.042791  1.135617  2.677271  0.551894  0.693255   \n",
              "13  0.709907  1.225747  0.703978  0.340741  0.927314  0.311449 -0.632622   \n",
              "14 -1.507811 -3.356466 -1.115297 -2.470574 -1.827662 -1.753905 -1.959737   \n",
              "15 -0.765645 -0.607651  0.222795  0.108677 -1.039242  0.564148  0.333824   \n",
              "16 -0.916210 -1.536024 -2.017177 -2.502937 -2.601159 -2.343201 -2.878085   \n",
              "17  0.698184  0.434111  2.309961  1.578595  1.883361  2.997817  1.976987   \n",
              "18 -0.082246  1.091428 -0.613424  0.610270  0.350521 -0.987134  0.342595   \n",
              "19 -1.055389 -1.310880 -1.039768 -1.168979  0.724548 -1.510419 -1.234207   \n",
              "20  1.141409  1.304245  1.522288  1.675341  0.397252  1.750833  2.051966   \n",
              "21 -0.538818  0.586065 -2.192485 -1.564645 -2.279820 -3.543623 -3.383000   \n",
              "22 -2.775065 -4.203029 -0.780914 -2.002720 -2.284354  0.623056 -0.109144   \n",
              "23 -0.911281 -0.365566 -0.823584 -0.064486  1.971936 -0.114171  0.171393   \n",
              "24 -1.192688 -0.917715 -2.049281 -2.455524 -0.039731 -1.747486 -2.889345   \n",
              "25  2.717887  0.909931  3.137932  1.249975  0.063883  2.788810  1.239225   \n",
              "26 -2.006858 -2.426982 -0.663574 -0.936704 -0.542015  0.822494  0.296223   \n",
              "27  0.144254  1.231921 -1.201708 -0.577319 -0.021917 -1.774721 -2.097731   \n",
              "28  0.533510  0.009472 -0.533459 -1.157177 -0.202409 -1.138000 -1.909447   \n",
              "29 -2.965374 -4.567176 -2.036735 -4.093701 -2.769812 -0.701682 -2.894238   \n",
              "30 -0.126261  0.242402 -0.100529  0.193236  0.123691  0.010732  0.253763   \n",
              "31 -0.203003 -2.065854  1.485137 -0.454519 -3.271129  2.929032  1.580775   \n",
              "32 -0.170411  0.817756 -1.549370 -0.198542  3.372428 -2.770976 -1.867213   \n",
              "33  1.056983 -0.918362  1.314848 -0.936329 -1.375221  0.923076 -0.768154   \n",
              "34 -0.945288  0.313596 -0.070864  0.308558 -1.194773  0.125123  0.560863   \n",
              "35 -0.477905 -0.927219 -0.855033 -1.406725 -2.244759 -1.311326 -1.808583   \n",
              "36  1.915903  2.708566  1.118015  1.780044  3.215603  1.059624  1.184901   \n",
              "37  0.261829 -1.632634 -0.466444 -2.325719 -2.295381 -0.940911 -2.883621   \n",
              "38  1.355107  0.919314  0.470841 -0.475362 -1.406592  0.488412 -1.113479   \n",
              "39 -1.627896 -1.019600 -1.554136 -1.185592 -1.095530 -1.316767 -1.376760   \n",
              "\n",
              "       7         8         9      ...     22026     22027     22028     22029  \\\n",
              "0   0.667998 -1.593489  0.029892  ... -1.397460 -0.511269 -1.322947 -2.000071   \n",
              "1   0.083391  0.904738  0.337820  ... -3.603525 -1.463744  1.853566  1.491078   \n",
              "2  -0.410883 -2.034216 -2.563028  ... -3.118834 -3.307258 -3.744647 -2.773463   \n",
              "3   4.875842  1.538924  2.753428  ...  2.202026  0.569497  1.508522  5.090201   \n",
              "4   1.845156  0.711661 -0.304123  ...  2.133809 -0.338683 -0.127999 -1.047067   \n",
              "5  -0.838817 -0.403448 -0.333990  ... -0.214000  1.380851  2.603960  1.077589   \n",
              "6  -1.607657  1.020348  1.676012  ... -3.169296 -2.035199 -0.381745 -3.382428   \n",
              "7   1.089391 -0.069834  0.235649  ...  2.098126  0.492819 -1.149343  2.129726   \n",
              "8   1.642874  1.613230  2.366586  ...  2.486706  1.535114  1.388396  4.369638   \n",
              "9  -0.568140 -2.774495 -2.557074  ...  1.778340  0.926865  0.633654  0.734015   \n",
              "10 -2.191570 -1.847258 -0.480837  ...  0.406286 -0.183989 -1.084103 -1.022032   \n",
              "11  1.599971  1.067873  3.096446  ...  1.745059  1.344284  1.345713  1.506843   \n",
              "12  1.556932 -0.633404 -0.042929  ... -7.918246 -7.361731 -4.885401 -0.081832   \n",
              "13 -0.020987  0.906913  0.070788  ...  3.046507  4.387523  2.531253 -1.679384   \n",
              "14 -0.231697 -1.457667 -3.152616  ... -1.925545 -4.114383 -3.540418 -1.899307   \n",
              "15 -0.384164  0.195428 -0.271623  ...  0.469176 -0.161480  0.557539  1.643084   \n",
              "16 -2.616313 -1.682562 -2.334147  ... -7.010486 -4.877500  0.139135 -1.409549   \n",
              "17  1.295207  2.179905  2.652618  ...  3.408414  2.280884  1.105175 -0.485143   \n",
              "18  0.502906 -1.622003 -1.926656  ... -0.559511 -0.282109  0.322870 -2.969235   \n",
              "19  1.365772 -1.260197 -2.340971  ... -2.600443 -2.990745 -0.476881  1.262864   \n",
              "20  1.020353  0.374137  1.206490  ... -4.259511 -2.586996  0.480080  0.131683   \n",
              "21 -3.307640 -2.332022 -3.924923  ... -3.524612 -3.142380 -1.173948 -1.799596   \n",
              "22 -0.960779 -0.202747  0.662340  ... -0.154793  0.581636  0.990883  1.433055   \n",
              "23  0.926213  0.988466  1.075888  ... -2.129088 -1.143755 -1.895462 -2.702327   \n",
              "24 -0.840095  0.282643 -1.385847  ... -3.649828 -4.372514 -3.805877  1.878096   \n",
              "25  0.182951  1.949033  2.450586  ... -0.031801  1.211252  1.548175 -0.039136   \n",
              "26  0.733845  2.581161  1.814191  ... -3.300594 -3.334415 -1.861742  0.574775   \n",
              "27 -2.193599  0.404127 -0.622162  ...  5.769835  4.779788  2.704664 -0.087257   \n",
              "28 -1.593867 -0.414914 -0.609689  ... -3.223550 -1.728264  0.087522 -1.150917   \n",
              "29 -2.888130  1.293492  0.672671  ...  4.986497  3.639989  1.534899  0.623492   \n",
              "30  0.366505 -0.347787  0.501027  ...  0.550031 -0.533428 -1.267669  1.029127   \n",
              "31 -1.751858  1.139544  3.909956  ... -1.206107  1.956743  4.292283  2.484532   \n",
              "32  2.476402 -1.332371 -3.636063  ...  2.034424  0.836375 -0.301352 -0.914800   \n",
              "33 -2.112033 -1.449397  0.384330  ... -0.742280 -2.700075 -2.685712  0.798542   \n",
              "34 -0.620393 -1.572583 -0.349201  ... -4.557334 -2.907023 -1.317662  0.682928   \n",
              "35 -2.487287 -0.619161 -1.688101  ... -3.981420 -2.581890 -0.228820  1.967203   \n",
              "36  1.573751  0.886471  1.882131  ...  4.586928  3.583880  1.717617  1.863586   \n",
              "37 -2.595577  1.186860 -0.873436  ... -2.504335 -2.352377 -0.915180 -0.062176   \n",
              "38 -1.945401  2.680645  1.229544  ...  1.439272  3.451933  4.880179  5.618734   \n",
              "39 -1.322437 -1.162760 -0.804233  ... -0.128889  0.016742  0.664634  1.713738   \n",
              "\n",
              "       22030     22031     22032     22033     22034     22035  \n",
              "0  -1.322005  0.120535 -2.822292 -3.571929 -1.831204 -0.456102  \n",
              "1   0.934197  0.476717 -3.217705 -3.100883 -3.194391 -1.938481  \n",
              "2  -2.697901 -1.867983 -2.842487 -3.153052 -3.668903 -4.108221  \n",
              "3   4.501404  2.556775  1.396895  2.946817  2.434524  1.748784  \n",
              "4  -1.852909 -2.353610  8.406331  5.815496  1.966806  0.357791  \n",
              "5  -0.324170 -1.066542 -0.100378 -0.497017 -0.400440  0.494570  \n",
              "6  -4.492195 -3.820596 -3.241980 -1.792638 -1.877472 -0.969380  \n",
              "7   4.286167  4.351320  4.431234  3.732273  1.683655  0.094166  \n",
              "8   4.231951  1.833858  1.599710  1.768548  1.600403  0.855522  \n",
              "9  -0.107875 -0.384738  3.826578  3.096500  3.167018  2.253150  \n",
              "10 -1.229713 -0.452375 -5.443149 -2.651542 -0.405870  0.120442  \n",
              "11  1.411527  0.082672  0.932769  1.826984  1.743816  1.275897  \n",
              "12 -0.485622 -0.848324 -3.845726 -5.412150 -6.345248 -6.445263  \n",
              "13  0.102583  1.658843  0.711618  1.807795  2.647501  2.895339  \n",
              "14 -2.838659 -3.128147 -0.311984 -0.175113 -2.526277 -3.221676  \n",
              "15  1.530191  0.883215  1.276431  1.882421  0.758384 -0.112706  \n",
              "16 -3.014063 -2.227121 -0.216580 -4.166958 -7.247024 -5.871346  \n",
              "17 -0.546122 -0.274678  5.068822  2.631715  1.994786  1.842607  \n",
              "18 -3.588451 -1.796116 -0.311627 -0.776920 -1.808996 -1.809158  \n",
              "19  0.409291 -0.475900 -3.196447 -0.587736 -1.874570 -2.770435  \n",
              "20 -0.677629 -0.639994  0.946282 -0.967851 -1.277294 -0.273834  \n",
              "21 -2.467522 -2.183247 -0.951505 -1.708750 -3.689249 -3.796523  \n",
              "22  3.159202  2.122869 -1.738707 -1.331605 -1.576223 -1.553290  \n",
              "23 -1.339391  0.635893 -3.040787 -3.119530 -1.869475 -1.356978  \n",
              "24  0.739517 -0.455757 -2.945199 -4.300382 -4.196514 -3.320418  \n",
              "25  0.564131  1.268537  0.013450 -0.723577 -0.223091  1.096123  \n",
              "26  0.976505  1.850826 -2.612722 -2.932277 -2.878022 -2.571367  \n",
              "27  0.151718  0.642830  2.533160  4.527023  4.914014  4.245656  \n",
              "28 -0.162192  1.403164 -4.094214 -2.879799 -2.956448 -2.786861  \n",
              "29 -0.305982 -1.209653  5.684371  5.634640  6.078718  4.582593  \n",
              "30  2.048517  1.995030 -3.248274 -0.128779  0.531445 -0.398239  \n",
              "31  1.299141  0.542315 -1.111295 -2.384773 -0.987417  1.671881  \n",
              "32  0.950611  2.327103 -0.723764  1.791617  0.980471 -0.898612  \n",
              "33  1.883990  2.687479 -0.751726  0.156107 -0.327650 -1.811943  \n",
              "34  2.194008  2.004348 -2.582339 -4.513578 -4.821950 -4.189694  \n",
              "35  0.101111 -1.248585 -7.726692 -3.765017 -2.797294 -2.767726  \n",
              "36  2.358673  2.650310  5.234509  4.814713  4.093631  3.603711  \n",
              "37 -0.557311 -0.787735 -0.144933 -0.742036 -2.180286 -1.936241  \n",
              "38  4.289414  2.972710 -2.154157 -0.675341  1.503938  2.870736  \n",
              "39  1.283099  1.400361 -1.422166  0.010186  1.034743  1.111142  \n",
              "\n",
              "[40 rows x 22036 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5962b897-bd39-4837-96d5-9f0aef11a2b8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22026</th>\n",
              "      <th>22027</th>\n",
              "      <th>22028</th>\n",
              "      <th>22029</th>\n",
              "      <th>22030</th>\n",
              "      <th>22031</th>\n",
              "      <th>22032</th>\n",
              "      <th>22033</th>\n",
              "      <th>22034</th>\n",
              "      <th>22035</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.717852</td>\n",
              "      <td>-1.489977</td>\n",
              "      <td>-0.788490</td>\n",
              "      <td>-0.766279</td>\n",
              "      <td>-0.672843</td>\n",
              "      <td>-0.485007</td>\n",
              "      <td>0.396688</td>\n",
              "      <td>0.667998</td>\n",
              "      <td>-1.593489</td>\n",
              "      <td>0.029892</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.397460</td>\n",
              "      <td>-0.511269</td>\n",
              "      <td>-1.322947</td>\n",
              "      <td>-2.000071</td>\n",
              "      <td>-1.322005</td>\n",
              "      <td>0.120535</td>\n",
              "      <td>-2.822292</td>\n",
              "      <td>-3.571929</td>\n",
              "      <td>-1.831204</td>\n",
              "      <td>-0.456102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.750366</td>\n",
              "      <td>-2.888667</td>\n",
              "      <td>-1.469767</td>\n",
              "      <td>-1.804638</td>\n",
              "      <td>-1.798576</td>\n",
              "      <td>-0.161826</td>\n",
              "      <td>-0.319420</td>\n",
              "      <td>0.083391</td>\n",
              "      <td>0.904738</td>\n",
              "      <td>0.337820</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.603525</td>\n",
              "      <td>-1.463744</td>\n",
              "      <td>1.853566</td>\n",
              "      <td>1.491078</td>\n",
              "      <td>0.934197</td>\n",
              "      <td>0.476717</td>\n",
              "      <td>-3.217705</td>\n",
              "      <td>-3.100883</td>\n",
              "      <td>-3.194391</td>\n",
              "      <td>-1.938481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.866373</td>\n",
              "      <td>2.680758</td>\n",
              "      <td>1.306910</td>\n",
              "      <td>0.890039</td>\n",
              "      <td>-0.175718</td>\n",
              "      <td>-0.730438</td>\n",
              "      <td>-0.993796</td>\n",
              "      <td>-0.410883</td>\n",
              "      <td>-2.034216</td>\n",
              "      <td>-2.563028</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.118834</td>\n",
              "      <td>-3.307258</td>\n",
              "      <td>-3.744647</td>\n",
              "      <td>-2.773463</td>\n",
              "      <td>-2.697901</td>\n",
              "      <td>-1.867983</td>\n",
              "      <td>-2.842487</td>\n",
              "      <td>-3.153052</td>\n",
              "      <td>-3.668903</td>\n",
              "      <td>-4.108221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.598113</td>\n",
              "      <td>2.346176</td>\n",
              "      <td>0.877477</td>\n",
              "      <td>3.045216</td>\n",
              "      <td>3.963654</td>\n",
              "      <td>1.601371</td>\n",
              "      <td>3.541291</td>\n",
              "      <td>4.875842</td>\n",
              "      <td>1.538924</td>\n",
              "      <td>2.753428</td>\n",
              "      <td>...</td>\n",
              "      <td>2.202026</td>\n",
              "      <td>0.569497</td>\n",
              "      <td>1.508522</td>\n",
              "      <td>5.090201</td>\n",
              "      <td>4.501404</td>\n",
              "      <td>2.556775</td>\n",
              "      <td>1.396895</td>\n",
              "      <td>2.946817</td>\n",
              "      <td>2.434524</td>\n",
              "      <td>1.748784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.567829</td>\n",
              "      <td>0.515177</td>\n",
              "      <td>1.447557</td>\n",
              "      <td>1.440498</td>\n",
              "      <td>1.315393</td>\n",
              "      <td>1.180648</td>\n",
              "      <td>1.389178</td>\n",
              "      <td>1.845156</td>\n",
              "      <td>0.711661</td>\n",
              "      <td>-0.304123</td>\n",
              "      <td>...</td>\n",
              "      <td>2.133809</td>\n",
              "      <td>-0.338683</td>\n",
              "      <td>-0.127999</td>\n",
              "      <td>-1.047067</td>\n",
              "      <td>-1.852909</td>\n",
              "      <td>-2.353610</td>\n",
              "      <td>8.406331</td>\n",
              "      <td>5.815496</td>\n",
              "      <td>1.966806</td>\n",
              "      <td>0.357791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.866487</td>\n",
              "      <td>0.280718</td>\n",
              "      <td>0.666179</td>\n",
              "      <td>0.489518</td>\n",
              "      <td>-0.428805</td>\n",
              "      <td>0.316380</td>\n",
              "      <td>0.760662</td>\n",
              "      <td>-0.838817</td>\n",
              "      <td>-0.403448</td>\n",
              "      <td>-0.333990</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.214000</td>\n",
              "      <td>1.380851</td>\n",
              "      <td>2.603960</td>\n",
              "      <td>1.077589</td>\n",
              "      <td>-0.324170</td>\n",
              "      <td>-1.066542</td>\n",
              "      <td>-0.100378</td>\n",
              "      <td>-0.497017</td>\n",
              "      <td>-0.400440</td>\n",
              "      <td>0.494570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.484973</td>\n",
              "      <td>-0.054756</td>\n",
              "      <td>2.056316</td>\n",
              "      <td>0.244368</td>\n",
              "      <td>-1.385715</td>\n",
              "      <td>2.112417</td>\n",
              "      <td>0.241465</td>\n",
              "      <td>-1.607657</td>\n",
              "      <td>1.020348</td>\n",
              "      <td>1.676012</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.169296</td>\n",
              "      <td>-2.035199</td>\n",
              "      <td>-0.381745</td>\n",
              "      <td>-3.382428</td>\n",
              "      <td>-4.492195</td>\n",
              "      <td>-3.820596</td>\n",
              "      <td>-3.241980</td>\n",
              "      <td>-1.792638</td>\n",
              "      <td>-1.877472</td>\n",
              "      <td>-0.969380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.396418</td>\n",
              "      <td>0.134505</td>\n",
              "      <td>0.452111</td>\n",
              "      <td>0.797386</td>\n",
              "      <td>0.627791</td>\n",
              "      <td>0.687820</td>\n",
              "      <td>1.199464</td>\n",
              "      <td>1.089391</td>\n",
              "      <td>-0.069834</td>\n",
              "      <td>0.235649</td>\n",
              "      <td>...</td>\n",
              "      <td>2.098126</td>\n",
              "      <td>0.492819</td>\n",
              "      <td>-1.149343</td>\n",
              "      <td>2.129726</td>\n",
              "      <td>4.286167</td>\n",
              "      <td>4.351320</td>\n",
              "      <td>4.431234</td>\n",
              "      <td>3.732273</td>\n",
              "      <td>1.683655</td>\n",
              "      <td>0.094166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.088526</td>\n",
              "      <td>0.553960</td>\n",
              "      <td>1.642410</td>\n",
              "      <td>2.349703</td>\n",
              "      <td>1.149499</td>\n",
              "      <td>2.607508</td>\n",
              "      <td>3.245725</td>\n",
              "      <td>1.642874</td>\n",
              "      <td>1.613230</td>\n",
              "      <td>2.366586</td>\n",
              "      <td>...</td>\n",
              "      <td>2.486706</td>\n",
              "      <td>1.535114</td>\n",
              "      <td>1.388396</td>\n",
              "      <td>4.369638</td>\n",
              "      <td>4.231951</td>\n",
              "      <td>1.833858</td>\n",
              "      <td>1.599710</td>\n",
              "      <td>1.768548</td>\n",
              "      <td>1.600403</td>\n",
              "      <td>0.855522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.237615</td>\n",
              "      <td>-0.130142</td>\n",
              "      <td>0.775877</td>\n",
              "      <td>-0.234693</td>\n",
              "      <td>-1.319983</td>\n",
              "      <td>-1.043659</td>\n",
              "      <td>-0.677686</td>\n",
              "      <td>-0.568140</td>\n",
              "      <td>-2.774495</td>\n",
              "      <td>-2.557074</td>\n",
              "      <td>...</td>\n",
              "      <td>1.778340</td>\n",
              "      <td>0.926865</td>\n",
              "      <td>0.633654</td>\n",
              "      <td>0.734015</td>\n",
              "      <td>-0.107875</td>\n",
              "      <td>-0.384738</td>\n",
              "      <td>3.826578</td>\n",
              "      <td>3.096500</td>\n",
              "      <td>3.167018</td>\n",
              "      <td>2.253150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3.183168</td>\n",
              "      <td>2.770886</td>\n",
              "      <td>0.827897</td>\n",
              "      <td>0.851232</td>\n",
              "      <td>-0.640553</td>\n",
              "      <td>-0.263272</td>\n",
              "      <td>-0.431441</td>\n",
              "      <td>-2.191570</td>\n",
              "      <td>-1.847258</td>\n",
              "      <td>-0.480837</td>\n",
              "      <td>...</td>\n",
              "      <td>0.406286</td>\n",
              "      <td>-0.183989</td>\n",
              "      <td>-1.084103</td>\n",
              "      <td>-1.022032</td>\n",
              "      <td>-1.229713</td>\n",
              "      <td>-0.452375</td>\n",
              "      <td>-5.443149</td>\n",
              "      <td>-2.651542</td>\n",
              "      <td>-0.405870</td>\n",
              "      <td>0.120442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.301304</td>\n",
              "      <td>0.930218</td>\n",
              "      <td>1.412441</td>\n",
              "      <td>2.474402</td>\n",
              "      <td>0.944745</td>\n",
              "      <td>2.213462</td>\n",
              "      <td>3.249793</td>\n",
              "      <td>1.599971</td>\n",
              "      <td>1.067873</td>\n",
              "      <td>3.096446</td>\n",
              "      <td>...</td>\n",
              "      <td>1.745059</td>\n",
              "      <td>1.344284</td>\n",
              "      <td>1.345713</td>\n",
              "      <td>1.506843</td>\n",
              "      <td>1.411527</td>\n",
              "      <td>0.082672</td>\n",
              "      <td>0.932769</td>\n",
              "      <td>1.826984</td>\n",
              "      <td>1.743816</td>\n",
              "      <td>1.275897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-0.137585</td>\n",
              "      <td>1.486551</td>\n",
              "      <td>-0.042791</td>\n",
              "      <td>1.135617</td>\n",
              "      <td>2.677271</td>\n",
              "      <td>0.551894</td>\n",
              "      <td>0.693255</td>\n",
              "      <td>1.556932</td>\n",
              "      <td>-0.633404</td>\n",
              "      <td>-0.042929</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.918246</td>\n",
              "      <td>-7.361731</td>\n",
              "      <td>-4.885401</td>\n",
              "      <td>-0.081832</td>\n",
              "      <td>-0.485622</td>\n",
              "      <td>-0.848324</td>\n",
              "      <td>-3.845726</td>\n",
              "      <td>-5.412150</td>\n",
              "      <td>-6.345248</td>\n",
              "      <td>-6.445263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.709907</td>\n",
              "      <td>1.225747</td>\n",
              "      <td>0.703978</td>\n",
              "      <td>0.340741</td>\n",
              "      <td>0.927314</td>\n",
              "      <td>0.311449</td>\n",
              "      <td>-0.632622</td>\n",
              "      <td>-0.020987</td>\n",
              "      <td>0.906913</td>\n",
              "      <td>0.070788</td>\n",
              "      <td>...</td>\n",
              "      <td>3.046507</td>\n",
              "      <td>4.387523</td>\n",
              "      <td>2.531253</td>\n",
              "      <td>-1.679384</td>\n",
              "      <td>0.102583</td>\n",
              "      <td>1.658843</td>\n",
              "      <td>0.711618</td>\n",
              "      <td>1.807795</td>\n",
              "      <td>2.647501</td>\n",
              "      <td>2.895339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-1.507811</td>\n",
              "      <td>-3.356466</td>\n",
              "      <td>-1.115297</td>\n",
              "      <td>-2.470574</td>\n",
              "      <td>-1.827662</td>\n",
              "      <td>-1.753905</td>\n",
              "      <td>-1.959737</td>\n",
              "      <td>-0.231697</td>\n",
              "      <td>-1.457667</td>\n",
              "      <td>-3.152616</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.925545</td>\n",
              "      <td>-4.114383</td>\n",
              "      <td>-3.540418</td>\n",
              "      <td>-1.899307</td>\n",
              "      <td>-2.838659</td>\n",
              "      <td>-3.128147</td>\n",
              "      <td>-0.311984</td>\n",
              "      <td>-0.175113</td>\n",
              "      <td>-2.526277</td>\n",
              "      <td>-3.221676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.765645</td>\n",
              "      <td>-0.607651</td>\n",
              "      <td>0.222795</td>\n",
              "      <td>0.108677</td>\n",
              "      <td>-1.039242</td>\n",
              "      <td>0.564148</td>\n",
              "      <td>0.333824</td>\n",
              "      <td>-0.384164</td>\n",
              "      <td>0.195428</td>\n",
              "      <td>-0.271623</td>\n",
              "      <td>...</td>\n",
              "      <td>0.469176</td>\n",
              "      <td>-0.161480</td>\n",
              "      <td>0.557539</td>\n",
              "      <td>1.643084</td>\n",
              "      <td>1.530191</td>\n",
              "      <td>0.883215</td>\n",
              "      <td>1.276431</td>\n",
              "      <td>1.882421</td>\n",
              "      <td>0.758384</td>\n",
              "      <td>-0.112706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-0.916210</td>\n",
              "      <td>-1.536024</td>\n",
              "      <td>-2.017177</td>\n",
              "      <td>-2.502937</td>\n",
              "      <td>-2.601159</td>\n",
              "      <td>-2.343201</td>\n",
              "      <td>-2.878085</td>\n",
              "      <td>-2.616313</td>\n",
              "      <td>-1.682562</td>\n",
              "      <td>-2.334147</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.010486</td>\n",
              "      <td>-4.877500</td>\n",
              "      <td>0.139135</td>\n",
              "      <td>-1.409549</td>\n",
              "      <td>-3.014063</td>\n",
              "      <td>-2.227121</td>\n",
              "      <td>-0.216580</td>\n",
              "      <td>-4.166958</td>\n",
              "      <td>-7.247024</td>\n",
              "      <td>-5.871346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.698184</td>\n",
              "      <td>0.434111</td>\n",
              "      <td>2.309961</td>\n",
              "      <td>1.578595</td>\n",
              "      <td>1.883361</td>\n",
              "      <td>2.997817</td>\n",
              "      <td>1.976987</td>\n",
              "      <td>1.295207</td>\n",
              "      <td>2.179905</td>\n",
              "      <td>2.652618</td>\n",
              "      <td>...</td>\n",
              "      <td>3.408414</td>\n",
              "      <td>2.280884</td>\n",
              "      <td>1.105175</td>\n",
              "      <td>-0.485143</td>\n",
              "      <td>-0.546122</td>\n",
              "      <td>-0.274678</td>\n",
              "      <td>5.068822</td>\n",
              "      <td>2.631715</td>\n",
              "      <td>1.994786</td>\n",
              "      <td>1.842607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.082246</td>\n",
              "      <td>1.091428</td>\n",
              "      <td>-0.613424</td>\n",
              "      <td>0.610270</td>\n",
              "      <td>0.350521</td>\n",
              "      <td>-0.987134</td>\n",
              "      <td>0.342595</td>\n",
              "      <td>0.502906</td>\n",
              "      <td>-1.622003</td>\n",
              "      <td>-1.926656</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.559511</td>\n",
              "      <td>-0.282109</td>\n",
              "      <td>0.322870</td>\n",
              "      <td>-2.969235</td>\n",
              "      <td>-3.588451</td>\n",
              "      <td>-1.796116</td>\n",
              "      <td>-0.311627</td>\n",
              "      <td>-0.776920</td>\n",
              "      <td>-1.808996</td>\n",
              "      <td>-1.809158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-1.055389</td>\n",
              "      <td>-1.310880</td>\n",
              "      <td>-1.039768</td>\n",
              "      <td>-1.168979</td>\n",
              "      <td>0.724548</td>\n",
              "      <td>-1.510419</td>\n",
              "      <td>-1.234207</td>\n",
              "      <td>1.365772</td>\n",
              "      <td>-1.260197</td>\n",
              "      <td>-2.340971</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.600443</td>\n",
              "      <td>-2.990745</td>\n",
              "      <td>-0.476881</td>\n",
              "      <td>1.262864</td>\n",
              "      <td>0.409291</td>\n",
              "      <td>-0.475900</td>\n",
              "      <td>-3.196447</td>\n",
              "      <td>-0.587736</td>\n",
              "      <td>-1.874570</td>\n",
              "      <td>-2.770435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.141409</td>\n",
              "      <td>1.304245</td>\n",
              "      <td>1.522288</td>\n",
              "      <td>1.675341</td>\n",
              "      <td>0.397252</td>\n",
              "      <td>1.750833</td>\n",
              "      <td>2.051966</td>\n",
              "      <td>1.020353</td>\n",
              "      <td>0.374137</td>\n",
              "      <td>1.206490</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.259511</td>\n",
              "      <td>-2.586996</td>\n",
              "      <td>0.480080</td>\n",
              "      <td>0.131683</td>\n",
              "      <td>-0.677629</td>\n",
              "      <td>-0.639994</td>\n",
              "      <td>0.946282</td>\n",
              "      <td>-0.967851</td>\n",
              "      <td>-1.277294</td>\n",
              "      <td>-0.273834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.538818</td>\n",
              "      <td>0.586065</td>\n",
              "      <td>-2.192485</td>\n",
              "      <td>-1.564645</td>\n",
              "      <td>-2.279820</td>\n",
              "      <td>-3.543623</td>\n",
              "      <td>-3.383000</td>\n",
              "      <td>-3.307640</td>\n",
              "      <td>-2.332022</td>\n",
              "      <td>-3.924923</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.524612</td>\n",
              "      <td>-3.142380</td>\n",
              "      <td>-1.173948</td>\n",
              "      <td>-1.799596</td>\n",
              "      <td>-2.467522</td>\n",
              "      <td>-2.183247</td>\n",
              "      <td>-0.951505</td>\n",
              "      <td>-1.708750</td>\n",
              "      <td>-3.689249</td>\n",
              "      <td>-3.796523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>-2.775065</td>\n",
              "      <td>-4.203029</td>\n",
              "      <td>-0.780914</td>\n",
              "      <td>-2.002720</td>\n",
              "      <td>-2.284354</td>\n",
              "      <td>0.623056</td>\n",
              "      <td>-0.109144</td>\n",
              "      <td>-0.960779</td>\n",
              "      <td>-0.202747</td>\n",
              "      <td>0.662340</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.154793</td>\n",
              "      <td>0.581636</td>\n",
              "      <td>0.990883</td>\n",
              "      <td>1.433055</td>\n",
              "      <td>3.159202</td>\n",
              "      <td>2.122869</td>\n",
              "      <td>-1.738707</td>\n",
              "      <td>-1.331605</td>\n",
              "      <td>-1.576223</td>\n",
              "      <td>-1.553290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.911281</td>\n",
              "      <td>-0.365566</td>\n",
              "      <td>-0.823584</td>\n",
              "      <td>-0.064486</td>\n",
              "      <td>1.971936</td>\n",
              "      <td>-0.114171</td>\n",
              "      <td>0.171393</td>\n",
              "      <td>0.926213</td>\n",
              "      <td>0.988466</td>\n",
              "      <td>1.075888</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.129088</td>\n",
              "      <td>-1.143755</td>\n",
              "      <td>-1.895462</td>\n",
              "      <td>-2.702327</td>\n",
              "      <td>-1.339391</td>\n",
              "      <td>0.635893</td>\n",
              "      <td>-3.040787</td>\n",
              "      <td>-3.119530</td>\n",
              "      <td>-1.869475</td>\n",
              "      <td>-1.356978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>-1.192688</td>\n",
              "      <td>-0.917715</td>\n",
              "      <td>-2.049281</td>\n",
              "      <td>-2.455524</td>\n",
              "      <td>-0.039731</td>\n",
              "      <td>-1.747486</td>\n",
              "      <td>-2.889345</td>\n",
              "      <td>-0.840095</td>\n",
              "      <td>0.282643</td>\n",
              "      <td>-1.385847</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.649828</td>\n",
              "      <td>-4.372514</td>\n",
              "      <td>-3.805877</td>\n",
              "      <td>1.878096</td>\n",
              "      <td>0.739517</td>\n",
              "      <td>-0.455757</td>\n",
              "      <td>-2.945199</td>\n",
              "      <td>-4.300382</td>\n",
              "      <td>-4.196514</td>\n",
              "      <td>-3.320418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2.717887</td>\n",
              "      <td>0.909931</td>\n",
              "      <td>3.137932</td>\n",
              "      <td>1.249975</td>\n",
              "      <td>0.063883</td>\n",
              "      <td>2.788810</td>\n",
              "      <td>1.239225</td>\n",
              "      <td>0.182951</td>\n",
              "      <td>1.949033</td>\n",
              "      <td>2.450586</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.031801</td>\n",
              "      <td>1.211252</td>\n",
              "      <td>1.548175</td>\n",
              "      <td>-0.039136</td>\n",
              "      <td>0.564131</td>\n",
              "      <td>1.268537</td>\n",
              "      <td>0.013450</td>\n",
              "      <td>-0.723577</td>\n",
              "      <td>-0.223091</td>\n",
              "      <td>1.096123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>-2.006858</td>\n",
              "      <td>-2.426982</td>\n",
              "      <td>-0.663574</td>\n",
              "      <td>-0.936704</td>\n",
              "      <td>-0.542015</td>\n",
              "      <td>0.822494</td>\n",
              "      <td>0.296223</td>\n",
              "      <td>0.733845</td>\n",
              "      <td>2.581161</td>\n",
              "      <td>1.814191</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.300594</td>\n",
              "      <td>-3.334415</td>\n",
              "      <td>-1.861742</td>\n",
              "      <td>0.574775</td>\n",
              "      <td>0.976505</td>\n",
              "      <td>1.850826</td>\n",
              "      <td>-2.612722</td>\n",
              "      <td>-2.932277</td>\n",
              "      <td>-2.878022</td>\n",
              "      <td>-2.571367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.144254</td>\n",
              "      <td>1.231921</td>\n",
              "      <td>-1.201708</td>\n",
              "      <td>-0.577319</td>\n",
              "      <td>-0.021917</td>\n",
              "      <td>-1.774721</td>\n",
              "      <td>-2.097731</td>\n",
              "      <td>-2.193599</td>\n",
              "      <td>0.404127</td>\n",
              "      <td>-0.622162</td>\n",
              "      <td>...</td>\n",
              "      <td>5.769835</td>\n",
              "      <td>4.779788</td>\n",
              "      <td>2.704664</td>\n",
              "      <td>-0.087257</td>\n",
              "      <td>0.151718</td>\n",
              "      <td>0.642830</td>\n",
              "      <td>2.533160</td>\n",
              "      <td>4.527023</td>\n",
              "      <td>4.914014</td>\n",
              "      <td>4.245656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.533510</td>\n",
              "      <td>0.009472</td>\n",
              "      <td>-0.533459</td>\n",
              "      <td>-1.157177</td>\n",
              "      <td>-0.202409</td>\n",
              "      <td>-1.138000</td>\n",
              "      <td>-1.909447</td>\n",
              "      <td>-1.593867</td>\n",
              "      <td>-0.414914</td>\n",
              "      <td>-0.609689</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.223550</td>\n",
              "      <td>-1.728264</td>\n",
              "      <td>0.087522</td>\n",
              "      <td>-1.150917</td>\n",
              "      <td>-0.162192</td>\n",
              "      <td>1.403164</td>\n",
              "      <td>-4.094214</td>\n",
              "      <td>-2.879799</td>\n",
              "      <td>-2.956448</td>\n",
              "      <td>-2.786861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>-2.965374</td>\n",
              "      <td>-4.567176</td>\n",
              "      <td>-2.036735</td>\n",
              "      <td>-4.093701</td>\n",
              "      <td>-2.769812</td>\n",
              "      <td>-0.701682</td>\n",
              "      <td>-2.894238</td>\n",
              "      <td>-2.888130</td>\n",
              "      <td>1.293492</td>\n",
              "      <td>0.672671</td>\n",
              "      <td>...</td>\n",
              "      <td>4.986497</td>\n",
              "      <td>3.639989</td>\n",
              "      <td>1.534899</td>\n",
              "      <td>0.623492</td>\n",
              "      <td>-0.305982</td>\n",
              "      <td>-1.209653</td>\n",
              "      <td>5.684371</td>\n",
              "      <td>5.634640</td>\n",
              "      <td>6.078718</td>\n",
              "      <td>4.582593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>-0.126261</td>\n",
              "      <td>0.242402</td>\n",
              "      <td>-0.100529</td>\n",
              "      <td>0.193236</td>\n",
              "      <td>0.123691</td>\n",
              "      <td>0.010732</td>\n",
              "      <td>0.253763</td>\n",
              "      <td>0.366505</td>\n",
              "      <td>-0.347787</td>\n",
              "      <td>0.501027</td>\n",
              "      <td>...</td>\n",
              "      <td>0.550031</td>\n",
              "      <td>-0.533428</td>\n",
              "      <td>-1.267669</td>\n",
              "      <td>1.029127</td>\n",
              "      <td>2.048517</td>\n",
              "      <td>1.995030</td>\n",
              "      <td>-3.248274</td>\n",
              "      <td>-0.128779</td>\n",
              "      <td>0.531445</td>\n",
              "      <td>-0.398239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-0.203003</td>\n",
              "      <td>-2.065854</td>\n",
              "      <td>1.485137</td>\n",
              "      <td>-0.454519</td>\n",
              "      <td>-3.271129</td>\n",
              "      <td>2.929032</td>\n",
              "      <td>1.580775</td>\n",
              "      <td>-1.751858</td>\n",
              "      <td>1.139544</td>\n",
              "      <td>3.909956</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.206107</td>\n",
              "      <td>1.956743</td>\n",
              "      <td>4.292283</td>\n",
              "      <td>2.484532</td>\n",
              "      <td>1.299141</td>\n",
              "      <td>0.542315</td>\n",
              "      <td>-1.111295</td>\n",
              "      <td>-2.384773</td>\n",
              "      <td>-0.987417</td>\n",
              "      <td>1.671881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>-0.170411</td>\n",
              "      <td>0.817756</td>\n",
              "      <td>-1.549370</td>\n",
              "      <td>-0.198542</td>\n",
              "      <td>3.372428</td>\n",
              "      <td>-2.770976</td>\n",
              "      <td>-1.867213</td>\n",
              "      <td>2.476402</td>\n",
              "      <td>-1.332371</td>\n",
              "      <td>-3.636063</td>\n",
              "      <td>...</td>\n",
              "      <td>2.034424</td>\n",
              "      <td>0.836375</td>\n",
              "      <td>-0.301352</td>\n",
              "      <td>-0.914800</td>\n",
              "      <td>0.950611</td>\n",
              "      <td>2.327103</td>\n",
              "      <td>-0.723764</td>\n",
              "      <td>1.791617</td>\n",
              "      <td>0.980471</td>\n",
              "      <td>-0.898612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.056983</td>\n",
              "      <td>-0.918362</td>\n",
              "      <td>1.314848</td>\n",
              "      <td>-0.936329</td>\n",
              "      <td>-1.375221</td>\n",
              "      <td>0.923076</td>\n",
              "      <td>-0.768154</td>\n",
              "      <td>-2.112033</td>\n",
              "      <td>-1.449397</td>\n",
              "      <td>0.384330</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.742280</td>\n",
              "      <td>-2.700075</td>\n",
              "      <td>-2.685712</td>\n",
              "      <td>0.798542</td>\n",
              "      <td>1.883990</td>\n",
              "      <td>2.687479</td>\n",
              "      <td>-0.751726</td>\n",
              "      <td>0.156107</td>\n",
              "      <td>-0.327650</td>\n",
              "      <td>-1.811943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>-0.945288</td>\n",
              "      <td>0.313596</td>\n",
              "      <td>-0.070864</td>\n",
              "      <td>0.308558</td>\n",
              "      <td>-1.194773</td>\n",
              "      <td>0.125123</td>\n",
              "      <td>0.560863</td>\n",
              "      <td>-0.620393</td>\n",
              "      <td>-1.572583</td>\n",
              "      <td>-0.349201</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.557334</td>\n",
              "      <td>-2.907023</td>\n",
              "      <td>-1.317662</td>\n",
              "      <td>0.682928</td>\n",
              "      <td>2.194008</td>\n",
              "      <td>2.004348</td>\n",
              "      <td>-2.582339</td>\n",
              "      <td>-4.513578</td>\n",
              "      <td>-4.821950</td>\n",
              "      <td>-4.189694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>-0.477905</td>\n",
              "      <td>-0.927219</td>\n",
              "      <td>-0.855033</td>\n",
              "      <td>-1.406725</td>\n",
              "      <td>-2.244759</td>\n",
              "      <td>-1.311326</td>\n",
              "      <td>-1.808583</td>\n",
              "      <td>-2.487287</td>\n",
              "      <td>-0.619161</td>\n",
              "      <td>-1.688101</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.981420</td>\n",
              "      <td>-2.581890</td>\n",
              "      <td>-0.228820</td>\n",
              "      <td>1.967203</td>\n",
              "      <td>0.101111</td>\n",
              "      <td>-1.248585</td>\n",
              "      <td>-7.726692</td>\n",
              "      <td>-3.765017</td>\n",
              "      <td>-2.797294</td>\n",
              "      <td>-2.767726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1.915903</td>\n",
              "      <td>2.708566</td>\n",
              "      <td>1.118015</td>\n",
              "      <td>1.780044</td>\n",
              "      <td>3.215603</td>\n",
              "      <td>1.059624</td>\n",
              "      <td>1.184901</td>\n",
              "      <td>1.573751</td>\n",
              "      <td>0.886471</td>\n",
              "      <td>1.882131</td>\n",
              "      <td>...</td>\n",
              "      <td>4.586928</td>\n",
              "      <td>3.583880</td>\n",
              "      <td>1.717617</td>\n",
              "      <td>1.863586</td>\n",
              "      <td>2.358673</td>\n",
              "      <td>2.650310</td>\n",
              "      <td>5.234509</td>\n",
              "      <td>4.814713</td>\n",
              "      <td>4.093631</td>\n",
              "      <td>3.603711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.261829</td>\n",
              "      <td>-1.632634</td>\n",
              "      <td>-0.466444</td>\n",
              "      <td>-2.325719</td>\n",
              "      <td>-2.295381</td>\n",
              "      <td>-0.940911</td>\n",
              "      <td>-2.883621</td>\n",
              "      <td>-2.595577</td>\n",
              "      <td>1.186860</td>\n",
              "      <td>-0.873436</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.504335</td>\n",
              "      <td>-2.352377</td>\n",
              "      <td>-0.915180</td>\n",
              "      <td>-0.062176</td>\n",
              "      <td>-0.557311</td>\n",
              "      <td>-0.787735</td>\n",
              "      <td>-0.144933</td>\n",
              "      <td>-0.742036</td>\n",
              "      <td>-2.180286</td>\n",
              "      <td>-1.936241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1.355107</td>\n",
              "      <td>0.919314</td>\n",
              "      <td>0.470841</td>\n",
              "      <td>-0.475362</td>\n",
              "      <td>-1.406592</td>\n",
              "      <td>0.488412</td>\n",
              "      <td>-1.113479</td>\n",
              "      <td>-1.945401</td>\n",
              "      <td>2.680645</td>\n",
              "      <td>1.229544</td>\n",
              "      <td>...</td>\n",
              "      <td>1.439272</td>\n",
              "      <td>3.451933</td>\n",
              "      <td>4.880179</td>\n",
              "      <td>5.618734</td>\n",
              "      <td>4.289414</td>\n",
              "      <td>2.972710</td>\n",
              "      <td>-2.154157</td>\n",
              "      <td>-0.675341</td>\n",
              "      <td>1.503938</td>\n",
              "      <td>2.870736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>-1.627896</td>\n",
              "      <td>-1.019600</td>\n",
              "      <td>-1.554136</td>\n",
              "      <td>-1.185592</td>\n",
              "      <td>-1.095530</td>\n",
              "      <td>-1.316767</td>\n",
              "      <td>-1.376760</td>\n",
              "      <td>-1.322437</td>\n",
              "      <td>-1.162760</td>\n",
              "      <td>-0.804233</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.128889</td>\n",
              "      <td>0.016742</td>\n",
              "      <td>0.664634</td>\n",
              "      <td>1.713738</td>\n",
              "      <td>1.283099</td>\n",
              "      <td>1.400361</td>\n",
              "      <td>-1.422166</td>\n",
              "      <td>0.010186</td>\n",
              "      <td>1.034743</td>\n",
              "      <td>1.111142</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40 rows  22036 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5962b897-bd39-4837-96d5-9f0aef11a2b8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5962b897-bd39-4837-96d5-9f0aef11a2b8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5962b897-bd39-4837-96d5-9f0aef11a2b8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a63846c6-7ff0-4c87-a3da-9bf8d57d368e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a63846c6-7ff0-4c87-a3da-9bf8d57d368e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a63846c6-7ff0-4c87-a3da-9bf8d57d368e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7e86e850-81d1-42f3-812c-1a7cb767d460\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7e86e850-81d1-42f3-812c-1a7cb767d460 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_test"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy9drkDDC0-O"
      },
      "source": [
        "## Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VRwom_0C0-O"
      },
      "source": [
        "\n",
        "\n",
        "### XG Boost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zb0px1wGC0-O"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMyB1Y65C0-P"
      },
      "source": [
        "#### Defining X and y:\n",
        "\n",
        "The input features (X) will be the preprocessed brain images, and the target labels (y) will be the corresponding genre categories for each image.\n",
        "\n",
        "**train_data.csv**: This CSV file contains the input features (X) for the training set. Each row represents a sample (brain image) in the training dataset, and each column corresponds to a feature (e.g., pixel intensity values). You'll need to preprocess this data and convert it into a format suitable for your machine learning model.\n",
        "\n",
        "**train_labels.csv**: This CSV file contains the target labels (y) for the training set. Each row corresponds to a sample in the training dataset, and there is a single column containing the genre category label for each sample. This column represents the target variable that the model will learn to predict.\n",
        "\n",
        "**test_data.csv**: This CSV file contains the input features (X) for the test set. Similar to the training data, each row represents a sample in the test dataset, and each column corresponds to a feature. You'll need to preprocess this data in the same way as the training data before making predictions.\n",
        "\n",
        "The labels (labels.csv) correspond to the correct musical genres.\n",
        "\n",
        "\n",
        "y_test => test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I7CrZLUrC0-P"
      },
      "outputs": [],
      "source": [
        "X = train_data\n",
        "y = train_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a_whruMPC0-Q"
      },
      "outputs": [],
      "source": [
        "# Train/Test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9nLz-k78C0-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f50c267-e9c8-4ebe-f67e-3b502975ff6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape:  (128, 22036)\n",
            "X_test shape:  (32, 22036)\n",
            "y_train shape:  (128, 1)\n",
            "y_test shape:  (32, 1)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"X_test shape: \", X_val.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"y_test shape: \", y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TOzier-kKyQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c3bWdzzPC0-R"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA for Dimensionality Reduction\n",
        "# Define the PCA and XGBoost models\n",
        "\n",
        "# Instantiating\n",
        "\n",
        "pca = PCA()\n",
        "xgb_model = XGBClassifier(objective='multi:softprob')"
      ],
      "metadata": {
        "id": "_bARcHMHQesp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERSONAL NOTE:**\n",
        "GridSearchCV is a technique for finding the optimal parameter values from a given set of parameters in a grid. It's essentially a cross-validation technique. The model as well as the parameters must be entered."
      ],
      "metadata": {
        "id": "RxVeZBd-HjRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST MODEL:\n",
        "# We gonna use hypermarameters WITHOUT the PCA.\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid for XGBoost\n",
        "grid_pilot = {\n",
        "    'pca__n_components': [50, 100, 100],  # Number of principal components\n",
        "    'xgb__learning_rate':[0.1, 0.05, 0.01],  # Learning rate\n",
        "    'xgb__n_estimators': [50, 100, 150],  # Number of trees\n",
        "    'xgb__max_depth': [3, 5, 7]  # Maximum depth of trees\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    xgb_model,\n",
        "    grid_pilot,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit data to Grid Search\n",
        "history_pilot = grid_search.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "history_pilot"
      ],
      "metadata": {
        "id": "qOfQqg7JPF-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best params for FIRST MODEL:\n",
        "best_params_01 = grid_search.best_params_"
      ],
      "metadata": {
        "id": "aJS5zYkCPvtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the validation set\n",
        "val_predictions = grid_search.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(\"Validation accuracy: {:.2f}\".format(val_accuracy))"
      ],
      "metadata": {
        "id": "ZQGMCuB0UkzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract CV results from the grid search\n",
        "cv_results = grid_search.cv_results_\n",
        "\n",
        "# Extract the mean and standard deviation of the cross-validated scores\n",
        "mean_scores = cv_results['mean_test_score']\n",
        "std_scores = cv_results['std_test_score']\n",
        "\n",
        "# Extract the hyperparameters for each iteration\n",
        "params = cv_results['params']\n",
        "\n",
        "# Plot the mean test scores with error bars\n",
        "plt.errorbar(range(len(mean_scores)), mean_scores, yerr=std_scores, fmt='o', markersize=5)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Mean Test Score')\n",
        "plt.title('Grid Search Cross-Validation Results')\n",
        "plt.xticks(range(len(mean_scores)), [str(param) for param in params], rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BJ0BvwTnUiLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECOND MODEL:\n",
        "# We gonna use hypermarameters PLUS the PCA.\n",
        "\n",
        "# Define the pipeline including PCA and XGBoost\n",
        "pipeline = Pipeline([\n",
        "    ('pca', pca),\n",
        "    ('xgb', xgb_model)\n",
        "])\n",
        "\n",
        "# Define the hyperparameter grid for XGBoost\n",
        "grid_01 = {\n",
        "    'pca__n_components': [50, 100, 100],\n",
        "    'xgb__max_depth':range(3,10,5),\n",
        "    'xgb__min_child_weight':range(1,6,2),\n",
        "    'xgb__n_estimators': [50, 100, 150],  # Number of trees\n",
        "    'xgb__max_depth': [3, 5, 7],  # Maximum depth of trees\n",
        "    'xgb__learning_rate': [0.03]  # Learning rate\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search_01 = GridSearchCV(\n",
        "    pipeline,\n",
        "    grid_01,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit data to Grid Search\n",
        "history_01 = grid_search_01.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "history_01"
      ],
      "metadata": {
        "id": "5nxTbR2NU5dL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a4945d99-efee-4eb5-f213-c8409ab3df74"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('pca', PCA()),\n",
              "                                       ('xgb',\n",
              "                                        XGBClassifier(base_score=None,\n",
              "                                                      booster=None,\n",
              "                                                      callbacks=None,\n",
              "                                                      colsample_bylevel=None,\n",
              "                                                      colsample_bynode=None,\n",
              "                                                      colsample_bytree=None,\n",
              "                                                      device=None,\n",
              "                                                      early_stopping_rounds=None,\n",
              "                                                      enable_categorical=False,\n",
              "                                                      eval_metric=None,\n",
              "                                                      feature_types=None,\n",
              "                                                      gamma=None,\n",
              "                                                      grow_policy=None,\n",
              "                                                      importance_type=None,\n",
              "                                                      interaction...\n",
              "                                                      monotone_constraints=None,\n",
              "                                                      multi_strategy=None,\n",
              "                                                      n_estimators=None,\n",
              "                                                      n_jobs=None,\n",
              "                                                      num_parallel_tree=None,\n",
              "                                                      objective='multi:softprob', ...))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={'pca__n_components': [50, 100, 100],\n",
              "                         'xgb__learning_rate': [0.03],\n",
              "                         'xgb__max_depth': [3, 5, 7],\n",
              "                         'xgb__min_child_weight': range(1, 6, 2),\n",
              "                         'xgb__n_estimators': [50, 100, 150]},\n",
              "             return_train_score=True, scoring='accuracy')"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA()),\n",
              "                                       (&#x27;xgb&#x27;,\n",
              "                                        XGBClassifier(base_score=None,\n",
              "                                                      booster=None,\n",
              "                                                      callbacks=None,\n",
              "                                                      colsample_bylevel=None,\n",
              "                                                      colsample_bynode=None,\n",
              "                                                      colsample_bytree=None,\n",
              "                                                      device=None,\n",
              "                                                      early_stopping_rounds=None,\n",
              "                                                      enable_categorical=False,\n",
              "                                                      eval_metric=None,\n",
              "                                                      feature_types=None,\n",
              "                                                      gamma=None,\n",
              "                                                      grow_policy=None,\n",
              "                                                      importance_type=None,\n",
              "                                                      interaction...\n",
              "                                                      monotone_constraints=None,\n",
              "                                                      multi_strategy=None,\n",
              "                                                      n_estimators=None,\n",
              "                                                      n_jobs=None,\n",
              "                                                      num_parallel_tree=None,\n",
              "                                                      objective=&#x27;multi:softprob&#x27;, ...))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;pca__n_components&#x27;: [50, 100, 100],\n",
              "                         &#x27;xgb__learning_rate&#x27;: [0.03],\n",
              "                         &#x27;xgb__max_depth&#x27;: [3, 5, 7],\n",
              "                         &#x27;xgb__min_child_weight&#x27;: range(1, 6, 2),\n",
              "                         &#x27;xgb__n_estimators&#x27;: [50, 100, 150]},\n",
              "             return_train_score=True, scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA()),\n",
              "                                       (&#x27;xgb&#x27;,\n",
              "                                        XGBClassifier(base_score=None,\n",
              "                                                      booster=None,\n",
              "                                                      callbacks=None,\n",
              "                                                      colsample_bylevel=None,\n",
              "                                                      colsample_bynode=None,\n",
              "                                                      colsample_bytree=None,\n",
              "                                                      device=None,\n",
              "                                                      early_stopping_rounds=None,\n",
              "                                                      enable_categorical=False,\n",
              "                                                      eval_metric=None,\n",
              "                                                      feature_types=None,\n",
              "                                                      gamma=None,\n",
              "                                                      grow_policy=None,\n",
              "                                                      importance_type=None,\n",
              "                                                      interaction...\n",
              "                                                      monotone_constraints=None,\n",
              "                                                      multi_strategy=None,\n",
              "                                                      n_estimators=None,\n",
              "                                                      n_jobs=None,\n",
              "                                                      num_parallel_tree=None,\n",
              "                                                      objective=&#x27;multi:softprob&#x27;, ...))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;pca__n_components&#x27;: [50, 100, 100],\n",
              "                         &#x27;xgb__learning_rate&#x27;: [0.03],\n",
              "                         &#x27;xgb__max_depth&#x27;: [3, 5, 7],\n",
              "                         &#x27;xgb__min_child_weight&#x27;: range(1, 6, 2),\n",
              "                         &#x27;xgb__n_estimators&#x27;: [50, 100, 150]},\n",
              "             return_train_score=True, scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA()),\n",
              "                (&#x27;xgb&#x27;,\n",
              "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None, device=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=None,\n",
              "                               feature_types=None, gamma=None, grow_policy=None,\n",
              "                               importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=None,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=None, max_leaves=None,\n",
              "                               min_child_weight=None, missing=nan,\n",
              "                               monotone_constraints=None, multi_strategy=None,\n",
              "                               n_estimators=None, n_jobs=None,\n",
              "                               num_parallel_tree=None,\n",
              "                               objective=&#x27;multi:softprob&#x27;, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Extracting CV results from the grid search\n",
        "# cv_results = grid_search_01.cv_results_\n",
        "\n",
        "# # Extracting the mean and standard deviation of the cross-validated scores\n",
        "# mean_scores = cv_results['mean_test_score']\n",
        "# std_scores = cv_results['std_test_score']\n",
        "\n",
        "# # Extracting the hyperparameters for each iteration\n",
        "# params = cv_results['params']\n",
        "\n",
        "# # Plotting the mean test scores with error bars\n",
        "# plt.errorbar(range(len(mean_scores)), mean_scores, yerr=std_scores, fmt='o', markersize=5)\n",
        "# plt.xlabel('Iteration')\n",
        "# plt.ylabel('Mean Test Score')\n",
        "# plt.title('Grid Search Cross-Validation Results')\n",
        "# plt.xticks(range(len(mean_scores)), [str(param) for param in params], rotation=45)\n",
        "# plt.grid(True)\n",
        "# plt.show();"
      ],
      "metadata": {
        "id": "VWH5O22JkFsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best params for SECOND MODEL:\n",
        "best_params_01 = grid_search_01.best_params_\n",
        "best_params_01"
      ],
      "metadata": {
        "id": "YHhngL_5NeGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbe4c45-cdfa-42cf-8ca6-a8ffca23c943"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pca__n_components': 50,\n",
              " 'xgb__learning_rate': 0.03,\n",
              " 'xgb__max_depth': 3,\n",
              " 'xgb__min_child_weight': 3,\n",
              " 'xgb__n_estimators': 150}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the validation set\n",
        "val_predictions = grid_search_01.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(\"Validation accuracy: {:.2f}\".format(val_accuracy))"
      ],
      "metadata": {
        "id": "eV7s6YJsQbZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2863e65-0186-4c9b-dae2-3dba14f9af13"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Grid search results:\")\n",
        "print(\"Best parameters found: \", grid_search_01.best_params_)\n",
        "print(\"Best cross-validation accuracy: {:.2f}\".format(grid_search_01.best_score_))"
      ],
      "metadata": {
        "id": "D9cUJh1sGFvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4fc965-1c34-43d5-8311-ba65cbe0a1e8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid search results:\n",
            "Best parameters found:  {'pca__n_components': 50, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 3, 'xgb__min_child_weight': 3, 'xgb__n_estimators': 150}\n",
            "Best cross-validation accuracy: 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_results_keys = history_01.cv_results_.keys()\n",
        "print(cv_results_keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V78uiCmgwFiB",
        "outputId": "757d9f74-6cf1-4912-a2df-3cbcaca80bbd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_pca__n_components', 'param_xgb__learning_rate', 'param_xgb__max_depth', 'param_xgb__min_child_weight', 'param_xgb__n_estimators', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'split2_train_score', 'split3_train_score', 'split4_train_score', 'mean_train_score', 'std_train_score'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract training and validation loss\n",
        "train_loss = -history_01.cv_results_['mean_train_score']\n",
        "val_loss = -history_01.cv_results_['mean_test_score']\n",
        "\n",
        "# Extract training and validation accuracy\n",
        "train_accuracy = history_01.cv_results_['mean_train_score']\n",
        "val_accuracy = history_01.cv_results_['mean_test_score']\n",
        "\n",
        "\n",
        "# Create epochs array\n",
        "epochs = np.arange(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss, marker='o', label='Training Loss', color='blue')\n",
        "#plt.plot(epochs, val_loss, marker='o', label='Validation Loss', color='orange')\n",
        "plt.plot(epochs, train_accuracy, marker='o', label='Training Accuracy', color='green')\n",
        "#plt.plot(epochs, val_accuracy, marker='o', label='Validation Accuracy', color='red')\n",
        "\n",
        "plt.title('Learning Curves')\n",
        "plt.xlabel('Epochs')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "vAPbUqNpoSkD",
        "outputId": "71d7914c-8a13-489e-f1d3-0da0362617b9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THIRD MODEL:\n",
        "# Now we gonna FINE TUNE the hypermarameters according to what we found before:\n",
        "\n",
        "pca = PCA(n_components=50)\n",
        "xgb_model = XGBClassifier(objective='multi:softprob',\n",
        "                          learning_rate=0.03,\n",
        "                          max_depth=3,\n",
        "                          min_child_weight=3,\n",
        "                          n_estimators=150,\n",
        "                          gamma=0.0,\n",
        "                          reg_alpha=0.01,\n",
        "                          n_jobs=4\n",
        "                          )\n",
        "\n",
        "# Define the pipeline including PCA and XGBoost\n",
        "pipeline_02 = Pipeline([\n",
        "    ('pca', pca),\n",
        "    ('xgb', xgb_model)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Fit data to Grid Search\n",
        "history_tuned = pipeline_02.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "history_tuned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "vk2NA686tSyg",
        "outputId": "3bd94371-620f-4b54-88fa-3f5de492b74e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('pca', PCA(n_components=50)),\n",
              "                ('xgb',\n",
              "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None, device=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=None,\n",
              "                               feature_types=None, gamma=0.0, grow_policy=None,\n",
              "                               importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=0.03,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=3, max_leaves=None, min_child_weight=3,\n",
              "                               missing=nan, monotone_constraints=None,\n",
              "                               multi_strategy=None, n_estimators=150, n_jobs=4,\n",
              "                               num_parallel_tree=None,\n",
              "                               objective='multi:softprob', ...))])"
            ],
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=50)),\n",
              "                (&#x27;xgb&#x27;,\n",
              "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None, device=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=None,\n",
              "                               feature_types=None, gamma=0.0, grow_policy=None,\n",
              "                               importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=0.03,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=3, max_leaves=None, min_child_weight=3,\n",
              "                               missing=nan, monotone_constraints=None,\n",
              "                               multi_strategy=None, n_estimators=150, n_jobs=4,\n",
              "                               num_parallel_tree=None,\n",
              "                               objective=&#x27;multi:softprob&#x27;, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=50)),\n",
              "                (&#x27;xgb&#x27;,\n",
              "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None, device=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=None,\n",
              "                               feature_types=None, gamma=0.0, grow_policy=None,\n",
              "                               importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=0.03,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=3, max_leaves=None, min_child_weight=3,\n",
              "                               missing=nan, monotone_constraints=None,\n",
              "                               multi_strategy=None, n_estimators=150, n_jobs=4,\n",
              "                               num_parallel_tree=None,\n",
              "                               objective=&#x27;multi:softprob&#x27;, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=50)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=0.0, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
              "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=150, n_jobs=4,\n",
              "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the validation set\n",
        "val_predictions_tuned = pipeline_02.predict(X_val)\n",
        "val_accuracy_tuned= accuracy_score(y_val, val_predictions)\n",
        "print(\"Validation accuracy: {:.2f}\".format(val_accuracy_tuned))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzl1LHvZRwwa",
        "outputId": "4d954811-35d8-4e13-e231-3e2bedda81e8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum accuracy found with XGBoost was: **Validation accuracy: 0.59**"
      ],
      "metadata": {
        "id": "6RC8HngjVRND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on test data:\n",
        "\n",
        "y_test = test_data\n",
        "predictions = pipeline_02.predict(y_test)\n",
        "\n",
        "# Show the predicted labels\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9JizUGtVcNB",
        "outputId": "ea449eb0-97a7-47b2-f82c-b1cf457a8fc2"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 4 3 0 4 2 2 3 0 3 4 3 3 3 0 3 2 1 3 0 1 0 1 2 2 1 0 3 0 0 1 0 1 0 2 4 4\n",
            " 4 0 0]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "260.797px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}