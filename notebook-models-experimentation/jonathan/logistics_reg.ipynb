{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inBFwoQZJJZF"
      },
      "source": [
        "#### Your objective is to classify fMRI brain images taken while listening to music in five different genres: label 0=Ambient Music, 1=Country Music, 2=Heavy Metal, 3=Rock 'n Roll, 4=Classical Symphonic. The data consists of train_data.csv, train_labels.csv, and test_data.csv, for a one-person subset of a larger 20-subject study, linked above.\n",
        "\n",
        "#### The training data (train_data.csv) consist of 160 event-related brain images (trials), corresponding to twenty 6-second music clips, four clips in each of the five genres, repeated in-order eight times (runs). The labels (train_labels.csv) correspond to the correct musical genres, listed above, for each of the 160 trials.\n",
        "\n",
        "#### There are 22036 features in each brain image, corresponding to blood-oxygenation levels at each 2mm-cubed 3D location within a section of the auditory cortex. In human brain imaging, there are often many more features (brain sites) than samples (trials), thus making the task a relatively challenging multiway classification problem.\n",
        "\n",
        "#### The testing data (test_data.csv) consists of 40 event-related brain images corresponding to novel 6-second music clips in the five genres. The test data is in randomized order with no labels. You must predict, using only the given brain images, the correct genre labels (0-4) for the 40 test trials.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25QjbIJtflqc"
      },
      "source": [
        "# Final Project\n",
        "\n",
        "# \"Classifying The Brain on Music\"\n",
        "\n",
        "Michael Casey, https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.01179/full\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBkfTOxuWxUY"
      },
      "source": [
        "## **1. Multi-Class Genre Classifier** [[12 points]](https://)\n",
        "\n",
        "#### Build a multi-class classifier for the 5 music genres. Your goal is to train a model to classify brain images into corresponding genre categories. You are free to choose any machine learning models from the class.\n",
        "\n",
        "#### **1-1. Hyper-parameter Search.** [[4 points]](https://) Demonstrate your hyperparameter search process using cross-validation. Provide details for at least one hyperparameter with 10 different possible values.\n",
        "\n",
        "#### **1-2. Model Training and Testing.** [[4 points]](https://) Following the hyperparameter search, train your model with the best combination of hyperparameters. Run the model on the test set and submit the results to the Kaggle competition. To get full marks, your model should outperform the baseline model, which is provided in Kaggle. You **must** show your test accuracy computed by Kaggle in this report.\n",
        "\n",
        "#### **1-3. Model Analysis.** [[4 points]](https://) Conduct a thorough analysis of your model, including:\n",
        "\n",
        "#### **1-3-1. Confusion Matrix:** Split the training set into train/validation sets. The data is organized into eight runs, in order, with each run repeating the same 20 music trials. You should split the data by run. Retrain your model using the best hyperparameter combination. Present the confusion matrix on the validation set.\n",
        "\n",
        "#### **1-3-2. Example Examination:** Examine four validation samples where your model fails to classify into the correct category. Display the true label and the predicted label. Looking at the confusion matrix, how might you explain your results from the perspectives of human brain data and music genre similarity?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA9cw0emNkjC"
      },
      "source": [
        "---\n",
        "\n",
        "## **A. Data Download**\n",
        "\n",
        "#### For your convenience, we have provided code to download the dataset, which includes true labels, training data (features), training labels, and testing data (features).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBfZLkUrZ9Xx"
      },
      "source": [
        "#### **A-1. Download Features and Labels.**\n",
        "\n",
        "#### Run the following code to download the brain features and labels of the music clips.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvMqDITgzRW2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjIWRNJzodXk"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1aFDPryEDcT5wg0k8NhWYpF8lulGmot5J # train data\n",
        "!gdown --id 11kgAdB_hkEcC4npCEWJcAOOmGe3495yY # train labels\n",
        "!gdown --id 1wXq56F6RIUtDzPceZegZAMA-JGW21Gqu # test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FA1IkzqxNFw7",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data.shape: (160, 22036)\n",
            "train_labels.shape: (160, 1)\n",
            "test_data.shape: (40, 22036)\n"
          ]
        }
      ],
      "source": [
        "# Data Import Method 1, with pandas\n",
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"../train_data.csv\", header=None)\n",
        "train_labels = pd.read_csv(\"../train_labels.csv\", header=None)\n",
        "test_data = pd.read_csv(\"../test_data.csv\", header=None)\n",
        "\n",
        "print('train_data.shape: {}'.format(train_data.shape))\n",
        "print('train_labels.shape: {}'.format(train_labels.shape))\n",
        "print('test_data.shape: {}'.format(test_data.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nFirst few rows of the dataset:\\n\")\n",
        "train_data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nDescriptive statistics for numerical columns:\\n\")\n",
        "train_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nInformation about the dataset:\\n\")\n",
        "print(train_data.info())\n",
        "\n",
        "print(\"\\nShape of the dataset (rows, columns):\\n\")\n",
        "print(train_data.shape)\n",
        "\n",
        "print(\"\\nData types of each column:\\n\")\n",
        "print(train_data.dtypes)\n",
        "\n",
        "# print(df['categorical_column'].value_counts())\n",
        "\n",
        "print(\"\\nNumber of missing values in each column:\\n\")\n",
        "print(train_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1: Split the data into training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.3, random_state=0) # 70% to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22026</th>\n",
              "      <th>22027</th>\n",
              "      <th>22028</th>\n",
              "      <th>22029</th>\n",
              "      <th>22030</th>\n",
              "      <th>22031</th>\n",
              "      <th>22032</th>\n",
              "      <th>22033</th>\n",
              "      <th>22034</th>\n",
              "      <th>22035</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>-2.314397</td>\n",
              "      <td>-3.039203</td>\n",
              "      <td>-1.922149</td>\n",
              "      <td>-2.733876</td>\n",
              "      <td>-4.540047</td>\n",
              "      <td>-1.237867</td>\n",
              "      <td>-2.188299</td>\n",
              "      <td>-3.827710</td>\n",
              "      <td>-0.692026</td>\n",
              "      <td>-0.160446</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.509140</td>\n",
              "      <td>-3.225268</td>\n",
              "      <td>-1.987008</td>\n",
              "      <td>-1.737181</td>\n",
              "      <td>-0.602613</td>\n",
              "      <td>-0.251485</td>\n",
              "      <td>-0.501410</td>\n",
              "      <td>-2.042777</td>\n",
              "      <td>-4.013156</td>\n",
              "      <td>-4.239245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>-1.307402</td>\n",
              "      <td>-0.978637</td>\n",
              "      <td>-1.899713</td>\n",
              "      <td>-1.711495</td>\n",
              "      <td>-1.775354</td>\n",
              "      <td>-2.644640</td>\n",
              "      <td>-2.332035</td>\n",
              "      <td>-2.357386</td>\n",
              "      <td>-2.515284</td>\n",
              "      <td>-3.280651</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.762068</td>\n",
              "      <td>-3.882897</td>\n",
              "      <td>-1.901563</td>\n",
              "      <td>0.797085</td>\n",
              "      <td>0.506390</td>\n",
              "      <td>0.288586</td>\n",
              "      <td>-5.840486</td>\n",
              "      <td>-4.511325</td>\n",
              "      <td>-3.731364</td>\n",
              "      <td>-3.447817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.331377</td>\n",
              "      <td>0.738699</td>\n",
              "      <td>1.551261</td>\n",
              "      <td>1.786645</td>\n",
              "      <td>1.339275</td>\n",
              "      <td>2.847958</td>\n",
              "      <td>2.920124</td>\n",
              "      <td>2.422327</td>\n",
              "      <td>2.576581</td>\n",
              "      <td>3.669024</td>\n",
              "      <td>...</td>\n",
              "      <td>3.582624</td>\n",
              "      <td>3.730723</td>\n",
              "      <td>2.714881</td>\n",
              "      <td>1.313629</td>\n",
              "      <td>0.517216</td>\n",
              "      <td>-2.136820</td>\n",
              "      <td>2.836182</td>\n",
              "      <td>4.440277</td>\n",
              "      <td>4.521889</td>\n",
              "      <td>3.791714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>-0.263944</td>\n",
              "      <td>-1.224477</td>\n",
              "      <td>0.527221</td>\n",
              "      <td>-1.141954</td>\n",
              "      <td>-1.276649</td>\n",
              "      <td>1.748549</td>\n",
              "      <td>-0.860848</td>\n",
              "      <td>-1.041420</td>\n",
              "      <td>4.880188</td>\n",
              "      <td>3.418812</td>\n",
              "      <td>...</td>\n",
              "      <td>5.961148</td>\n",
              "      <td>6.062754</td>\n",
              "      <td>3.741080</td>\n",
              "      <td>0.162845</td>\n",
              "      <td>0.907009</td>\n",
              "      <td>-0.092087</td>\n",
              "      <td>7.276143</td>\n",
              "      <td>5.977975</td>\n",
              "      <td>5.233674</td>\n",
              "      <td>5.221163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.045658</td>\n",
              "      <td>0.158404</td>\n",
              "      <td>1.430457</td>\n",
              "      <td>-0.088408</td>\n",
              "      <td>-3.513397</td>\n",
              "      <td>1.624462</td>\n",
              "      <td>-0.278195</td>\n",
              "      <td>-4.185827</td>\n",
              "      <td>1.390569</td>\n",
              "      <td>2.215716</td>\n",
              "      <td>...</td>\n",
              "      <td>1.625480</td>\n",
              "      <td>2.924401</td>\n",
              "      <td>3.821515</td>\n",
              "      <td>0.884437</td>\n",
              "      <td>0.617778</td>\n",
              "      <td>0.485366</td>\n",
              "      <td>-1.133856</td>\n",
              "      <td>0.996806</td>\n",
              "      <td>1.658188</td>\n",
              "      <td>3.123640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-1.776406</td>\n",
              "      <td>-1.757771</td>\n",
              "      <td>-2.242440</td>\n",
              "      <td>-1.522027</td>\n",
              "      <td>-0.109872</td>\n",
              "      <td>-2.347088</td>\n",
              "      <td>-0.985578</td>\n",
              "      <td>-0.174761</td>\n",
              "      <td>-2.351364</td>\n",
              "      <td>-1.260209</td>\n",
              "      <td>...</td>\n",
              "      <td>0.895446</td>\n",
              "      <td>-2.435271</td>\n",
              "      <td>-2.270254</td>\n",
              "      <td>2.291000</td>\n",
              "      <td>2.656067</td>\n",
              "      <td>2.051590</td>\n",
              "      <td>2.743168</td>\n",
              "      <td>3.810833</td>\n",
              "      <td>1.372110</td>\n",
              "      <td>-2.026703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>-3.380314</td>\n",
              "      <td>-2.831648</td>\n",
              "      <td>-2.556751</td>\n",
              "      <td>-1.693914</td>\n",
              "      <td>-1.438040</td>\n",
              "      <td>-2.200296</td>\n",
              "      <td>-0.444568</td>\n",
              "      <td>0.689061</td>\n",
              "      <td>-2.497783</td>\n",
              "      <td>-2.217416</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.526276</td>\n",
              "      <td>-12.696812</td>\n",
              "      <td>-6.558849</td>\n",
              "      <td>-0.673435</td>\n",
              "      <td>-3.066120</td>\n",
              "      <td>-2.289176</td>\n",
              "      <td>-3.955882</td>\n",
              "      <td>-5.050390</td>\n",
              "      <td>-8.200123</td>\n",
              "      <td>-9.937320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>-0.168596</td>\n",
              "      <td>0.214571</td>\n",
              "      <td>-0.359398</td>\n",
              "      <td>0.949367</td>\n",
              "      <td>0.722810</td>\n",
              "      <td>-0.642784</td>\n",
              "      <td>0.963331</td>\n",
              "      <td>0.915781</td>\n",
              "      <td>-1.779895</td>\n",
              "      <td>-0.446726</td>\n",
              "      <td>...</td>\n",
              "      <td>4.444767</td>\n",
              "      <td>6.237965</td>\n",
              "      <td>5.752860</td>\n",
              "      <td>1.637919</td>\n",
              "      <td>1.536919</td>\n",
              "      <td>1.328895</td>\n",
              "      <td>6.336852</td>\n",
              "      <td>4.586332</td>\n",
              "      <td>5.405281</td>\n",
              "      <td>7.623970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>-0.560468</td>\n",
              "      <td>-1.993829</td>\n",
              "      <td>1.343158</td>\n",
              "      <td>-0.582546</td>\n",
              "      <td>-0.861013</td>\n",
              "      <td>2.389700</td>\n",
              "      <td>0.666740</td>\n",
              "      <td>0.433878</td>\n",
              "      <td>2.166772</td>\n",
              "      <td>2.245840</td>\n",
              "      <td>...</td>\n",
              "      <td>2.941923</td>\n",
              "      <td>3.062529</td>\n",
              "      <td>1.659200</td>\n",
              "      <td>1.940238</td>\n",
              "      <td>3.031249</td>\n",
              "      <td>2.814192</td>\n",
              "      <td>0.097506</td>\n",
              "      <td>1.515483</td>\n",
              "      <td>2.866854</td>\n",
              "      <td>2.546117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>-1.608385</td>\n",
              "      <td>0.017438</td>\n",
              "      <td>-2.671780</td>\n",
              "      <td>-0.940511</td>\n",
              "      <td>0.194179</td>\n",
              "      <td>-3.392766</td>\n",
              "      <td>-1.828320</td>\n",
              "      <td>-0.390330</td>\n",
              "      <td>-3.385409</td>\n",
              "      <td>-3.726256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.308628</td>\n",
              "      <td>1.020752</td>\n",
              "      <td>-0.833266</td>\n",
              "      <td>-2.377312</td>\n",
              "      <td>-1.308513</td>\n",
              "      <td>0.402756</td>\n",
              "      <td>-1.284950</td>\n",
              "      <td>-0.518043</td>\n",
              "      <td>0.516541</td>\n",
              "      <td>0.619020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>112 rows Ã— 22036 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6      \\\n",
              "118 -2.314397 -3.039203 -1.922149 -2.733876 -4.540047 -1.237867 -2.188299   \n",
              "95  -1.307402 -0.978637 -1.899713 -1.711495 -1.775354 -2.644640 -2.332035   \n",
              "55   0.331377  0.738699  1.551261  1.786645  1.339275  2.847958  2.920124   \n",
              "109 -0.263944 -1.224477  0.527221 -1.141954 -1.276649  1.748549 -0.860848   \n",
              "18   1.045658  0.158404  1.430457 -0.088408 -3.513397  1.624462 -0.278195   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "9   -1.776406 -1.757771 -2.242440 -1.522027 -0.109872 -2.347088 -0.985578   \n",
              "103 -3.380314 -2.831648 -2.556751 -1.693914 -1.438040 -2.200296 -0.444568   \n",
              "67  -0.168596  0.214571 -0.359398  0.949367  0.722810 -0.642784  0.963331   \n",
              "117 -0.560468 -1.993829  1.343158 -0.582546 -0.861013  2.389700  0.666740   \n",
              "47  -1.608385  0.017438 -2.671780 -0.940511  0.194179 -3.392766 -1.828320   \n",
              "\n",
              "        7         8         9      ...      22026      22027     22028  \\\n",
              "118 -3.827710 -0.692026 -0.160446  ...  -3.509140  -3.225268 -1.987008   \n",
              "95  -2.357386 -2.515284 -3.280651  ...  -4.762068  -3.882897 -1.901563   \n",
              "55   2.422327  2.576581  3.669024  ...   3.582624   3.730723  2.714881   \n",
              "109 -1.041420  4.880188  3.418812  ...   5.961148   6.062754  3.741080   \n",
              "18  -4.185827  1.390569  2.215716  ...   1.625480   2.924401  3.821515   \n",
              "..        ...       ...       ...  ...        ...        ...       ...   \n",
              "9   -0.174761 -2.351364 -1.260209  ...   0.895446  -2.435271 -2.270254   \n",
              "103  0.689061 -2.497783 -2.217416  ... -11.526276 -12.696812 -6.558849   \n",
              "67   0.915781 -1.779895 -0.446726  ...   4.444767   6.237965  5.752860   \n",
              "117  0.433878  2.166772  2.245840  ...   2.941923   3.062529  1.659200   \n",
              "47  -0.390330 -3.385409 -3.726256  ...   0.308628   1.020752 -0.833266   \n",
              "\n",
              "        22029     22030     22031     22032     22033     22034     22035  \n",
              "118 -1.737181 -0.602613 -0.251485 -0.501410 -2.042777 -4.013156 -4.239245  \n",
              "95   0.797085  0.506390  0.288586 -5.840486 -4.511325 -3.731364 -3.447817  \n",
              "55   1.313629  0.517216 -2.136820  2.836182  4.440277  4.521889  3.791714  \n",
              "109  0.162845  0.907009 -0.092087  7.276143  5.977975  5.233674  5.221163  \n",
              "18   0.884437  0.617778  0.485366 -1.133856  0.996806  1.658188  3.123640  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "9    2.291000  2.656067  2.051590  2.743168  3.810833  1.372110 -2.026703  \n",
              "103 -0.673435 -3.066120 -2.289176 -3.955882 -5.050390 -8.200123 -9.937320  \n",
              "67   1.637919  1.536919  1.328895  6.336852  4.586332  5.405281  7.623970  \n",
              "117  1.940238  3.031249  2.814192  0.097506  1.515483  2.866854  2.546117  \n",
              "47  -2.377312 -1.308513  0.402756 -1.284950 -0.518043  0.516541  0.619020  \n",
              "\n",
              "[112 rows x 22036 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2: Normalize the features using StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Seems to decrease accuracy.\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.50297059, -1.61024443, -1.17233137, ..., -0.56837978,\n",
              "        -1.09890874, -1.20940757],\n",
              "       [-0.83243981, -0.43640166, -1.15877359, ..., -1.32022538,\n",
              "        -1.0143281 , -0.9682414 ],\n",
              "       [ 0.25877859,  0.54191367,  0.92661449, ...,  1.40616335,\n",
              "         1.4629022 ,  1.23780979],\n",
              "       ...,\n",
              "       [-0.07413977,  0.24333353, -0.22797708, ...,  1.45064755,\n",
              "         1.72805405,  2.40558616],\n",
              "       [-0.33507725, -1.01472619,  0.80086001, ...,  0.51535911,\n",
              "         0.96614001,  0.85824792],\n",
              "       [-1.03285619,  0.13103251, -1.6253256 , ..., -0.10399166,\n",
              "         0.26068912,  0.27101714]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3: One-hot encode the target variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_scaled = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = encoder.transform(y_test.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Create a simple sequential model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import make_scorer, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters:  {'C': 10}\n",
            "Best Score:  0.6600790513833992\n",
            "Test Accuracy: 0.708\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(max_iter=5000)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [10, 15, 20, 25, 30, 60, 100, 200]\n",
        "}\n",
        "grid_search = GridSearchCV(logreg, param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(X_train, y_train[0].tolist())\n",
        "\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "best_logreg = grid_search.best_estimator_\n",
        "best_logreg.fit(X_train, y_train[0].tolist())\n",
        "test_acc = accuracy_score(y_test, best_logreg.predict(X_test))\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters:  {'C': 0.01}\n",
            "Best Score:  0.7596837944664031\n",
            "Test Accuracy: 0.750\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pca = PCA(n_components=90)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "logreg = LogisticRegression(max_iter=5000)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "grid_search = GridSearchCV(logreg, param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(X_train_pca, y_train[0].tolist())\n",
        "\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "best_logreg = grid_search.best_estimator_\n",
        "best_logreg.fit(X_train_pca, y_train[0].tolist())\n",
        "test_acc = accuracy_score(y_test, best_logreg.predict(X_test_pca))\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters:  {'C': 1}\n",
            "Best Score:  0.6869565217391305\n",
            "Test Accuracy: 0.688\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pca = PCA(n_components=90)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "grid_search = GridSearchCV(logreg, param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(X_train_pca, y_train[0].tolist())\n",
        "\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "best_logreg = grid_search.best_estimator_\n",
        "best_logreg.fit(X_train_pca, y_train[0].tolist())\n",
        "test_acc = accuracy_score(y_test, best_logreg.predict(X_test_pca))\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
